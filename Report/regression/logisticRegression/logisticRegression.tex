\section{Logistic Regression}\label{sc:logisticRegression}
We sometimes want to classify a response variable that has two classes. Examples of such classes could be being accepted or rejected into school or the market going up or down. Therefore our target class Y should be seen as a binary class. Where $0$ suggest Reject (Negative Class) and $1$ suggest Accept (Positive Class).
\subsection{Theory}
So we want to model the probability of the default class. If we are modeling people’s gender as male or female based on their shoe size, then the first class could be male and could be written as the probability of male given a person’s shoe size.
\begin{align}
P(x) = P(gender=male|ShoeSize)
\end{align}
or we could say we are modeling the probability that the input (X) belongs to our default class and that is Y=1.
\begin{align}\label{fo:probability}
P(x) = P(Y=1|X)
\end{align}
We then use the logistic function seen in Equation \ref{fo:LogisticFunction} because it will give outputs between $0$ and $1$ for all values of X and will always produce an S-shaped curve.%TODO Add S-Curve here example p. 131
\begin{align}\label{fo:LogisticFunction}
P(x) = \dfrac{ e^{\beta_0 + \beta_1 X}}{  1 + e^{\beta_0 + \beta_1 X}}
\end{align}
If we move things a little (the $e$ can be removed from one side by adding a natural logarithm $ln$ to the other) we get Equation \ref{fo:logit}. Here we can see that it still a linear model, but we are modeling the probabilities on a non-linear scale. The ratio on the left in the equation is called log odds of the default class. This is calculated as a ratio of the probability of the event divided by the probability of not the event, e.g. $\frac{0.5}{1-0.5}$ which has the odds of $1$.
 \begin{align}\label{fo:logit}
\log\bigg( \dfrac{ P(X)}{1-P(x)} \bigg) = \beta_0 + \beta_1 X
\end{align}

As an example; assume $\beta_0 = -10 $ and $ \beta_1 = 0.4 $, using the equations as defined above, plug in the numbers and find the probability of "male" given the shoe size of 24 defined as $P(x) = P(gender=male|ShoeSize=24)$ resulting in $P(x) = \dfrac{ e^{-10 + 0.4*24}}{1 + e^{-10 + 0.4*24}} = 0.40$, $0.4$ probability. A threshold can be created for our binary classifier $0$ if $p(male) < 0.5$ and $1$ if $ p(male) >= 0.5$

To estimate the regression coefficients of the model in Equation \ref{fo:logit} we use a technique called maximum likelihood. The goal is to find $\beta_0$ and $\beta_1$, such that when estimating $P(x)$, the result corresponds as closely as possible to the observed individual data of gender given a shoe size. So your coefficients should yield a number close to 0 if the person is not male and a number close to 1 if the person is male. We determine these coefficients by maximizing the likelihood function.

%TODO: If someone can do the maximum likelihood function in latex, please feel free.

\subsection{Results}
\subsubsection*{LAB 4.6.1 and 4.6.2}
The lab 4.6.1\footnote{Appendix 3 - 4.6.1 The Stock Market Data} and 4.6.2\footnote{Appendix 4 - 4.6.2 Logistic Regression} uses stock market data and trying to predict if the market goes up or down base on the Daily percentage returns for stock index between 2001 and 2005.
Displaying the data to get a sense of it:
\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}183}]:}    Year   Lag1   Lag2   Lag3   Lag4   Lag5  Volume  Today Direction
1  2001  0.381 -0.192 -2.624 -1.055  5.010  1.1913  0.959        Up
\end{Verbatim}
Since some of the headers are not very clearly named, following is a quick explanation\footnote{https://rdrr.io/cran/ISLR/man/Smarket.html}. Each $LagX$ denotes the percentage return $X$ days earlier, that is an indicator of the state of the market $X$ days before Today. The remaining unclear fields would be Volume, Volume of shares traded in billions, Today, percentage return for today, and Direction, whether the day had a positive or negative return. Because the classifier only works with digits the Today Direction column is changed into a number and split the data set into a y and X.

Now that y and X is established using the statsmodels library a logistic regression can be made and fit to the model. Then assign Direction[Up] as a $1$ and $0$ as Direction[Down].

The detailed information on the fit:
\begin{lstlisting}
               coef       std err     z          P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -0.1260      0.241     -0.523      0.601      -0.598       0.346
Lag1          -0.0731      0.050     -1.457      0.145      -0.171       0.025
\end{lstlisting}
A negative for the Lag1 coefficient tell that if the market had a positive return yesterday, then it is less likely to go up today. Because the smallest P value is relatively large ($0.145$) there is no real evidence of good association between Lag1 and Direction.

To get the accuracy of the model we take the mean of data we predicted correctly model hence it only predicted the movement of the market $52.2\%$ of the time.

\noindent\textit{0.52159999999999995}

To create and validate a model, the data is split into two parts, a train set (all before 2005) and a test set (all after 2005). This gives the possibility of using cross-validation, when the model has been trained using the train set.

After the model have been trained, detailed information about the fit can be observed. A negative for the Lag1 coefficient tells that if the market had a positive return yesterday, then it is less likely to go up today. But because the P value kind of big still there is no real evidence of good association between Lag1 and Direction.

\begin{lstlisting}[language=Python]
                coef      std err       z        P>|z|      [0.025       0.975]
------------------------------------------------------------------------------
Intercept      0.1912      0.334      0.573      0.567      -0.463       0.845
Lag1          -0.0542      0.052     -1.046      0.295      -0.156       0.047
Lag2          -0.0458      0.052     -0.884      0.377      -0.147       0.056
Lag3           0.0072      0.052      0.139      0.889      -0.094       0.108
Lag4           0.0064      0.052      0.125      0.901      -0.095       0.108
Lag5          -0.0042      0.051     -0.083      0.934      -0.104       0.096
Volume        -0.1163      0.240     -0.485      0.628      -0.586       0.353
==============================================================================
\end{lstlisting}

The error is still $1 - 0.48 = 0.52$. The P-values, Lag3 to Lag5, in the model is relatively high, by retraining the model without them. This should lead to a more efficient model. 

Because $1 - 0.44 = 0.56$ there is a $0.56$ probability of an error. Hence it shows that there is a $56\%$ chance that the market should go up on days when the model predict it should not. But the dataset is really too small to show if it is trend or just random chance.

\subsection{Conclusion}
Logistic Regression is a good classifier when predicting a binary outcome. When a binary outcome is insufficient the discriminant analysis, is a good choice for multiple-class classification.
