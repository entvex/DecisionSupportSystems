\section{Bootstrap}\label{ch:bootstrap}

A second method for producing additional information from a limited data set is to use the bootstrap method. Bootstrap is performed by shaking up the data and plucking out random observations, while allowing for the same observation to be included multiple times, to create a new sample. The process can then be repeated as many times as needed until the desired amount of data becomes available. 

\subsection{Theory}

\iffalse
A good explanation https://stats.stackexchange.com/questions/26088/explaining-to-laypeople-why-bootstrapping-works
\fi

Bootstrapping works on the assumption that distribution parameters are transitive. Such that a random number generated by taking the mean of N samples from a dataset containing N samples, but allowing for replacement, will be equally valid as a random number generated from the original population.

As an example dataset X contains N observations from the original population $\theta$ we resample from X until a new dataset Y with N elements is created. Assuming $\theta$ follows a Gaussian distribution then the mean of Y is a random number generated from the distribution of X which is an estimate of the actual distribution of $\theta$ where each observation is generated from $\theta$. Therefore the mean of Y is an estimated observation from $\theta$.

Assuming we have sufficient data then the estimated distribution can be used for generating a new, more populated, data set without sacrificing much of the quality. As the observed distribution is an estimate of the population distribution, the resulting data set will have a distribution which is an estimate of the observations.

\iffalse % Olaf: A second attempt at writing this section. Keeping it for review purposes.
Bootstrapping works by resampling the initial data with replacement, until a new data set has been created from the original. The new dataset will then contain a random sample of data with the same distribution as the original, allowing for some deviation in the parameters. By repeating this B times and averaging the distribution parameters over all B samples we get a better estimate of the original population as if we had gotten more observations.

As an example, assuming we have a normally distributed population of N elements then by taking a random resampling from that dataset until a new dataset, also containing N elements, is gained. The mean of that new dataset can then be seen as an estimated new observation from the original population. We then repeat this process B times and in the end we have a new data set of both resampled means and variances. Since both were generated using the original set of observations the set of resamples will also be normally distributed with a mean and variance close to the original population.
\fi

The standard error of the resampled data can be estimated through the sample standard deviation, as seen in Equation \ref{fo:bootstrapStandardError}. $\hat{\theta}$ denotes the bootstrap data of the estimated population $\theta$. $\hat{\theta_{b}}$ where $b=1,\dots,B$ denotes each bootsrap dataset. $\bar{\theta}$ denotes the mean of the bootstrap data; $\bar{\theta} = \frac{1}{B} \sum_{b=1}^{B}(\hat{\theta_{b}})$.
\begin{align}\label{fo:bootstrapStandardError}
	SE(\hat{\theta}) = \sqrt{\frac{1}{B-1} \sum_{b=1}^{B}(\hat{\theta_{b}} - \bar{\theta})} 
\end{align}

%TODO What about uncorrelated sample population data? What about using bootstrap to estimate prediction error?

\subsection{Results}

Before starting the lab a couple of functions were defined. The one referred to in the results is $boot\_lm$ and is defined below. The function takes in a data set that will be used as the basis for the bootstrap, an $r\_formula$ for defining how the least squares regression should be calculated, and $B$ how many resamples should be taken.

\begin{lstlisting}[language=Python]
def boot_lm(data, r_formula, B):
  # data the data to bootstrap
  # r_formula the least squares regression R formula to use on the data
  # B the number of iterations of bootstrapping
  coef = []
  for i in range(0, B):
    # Take a least squares regression of each resample using r_formula
    v = formula.ols(r_formula, data=data.sample(frac=1.0, replace=True)).fit()
    coef.append(v.params)
  return coef
\end{lstlisting}

In lab 5.3.4. on Bootstrap we will take a small-ish data sample and use the bootstrapping method to increase the data quality. First to examine the process on the Portfolio data set, and then apply the method to the auto data set and review the results for linear and quadratic regression compared to the original.

\begin{lstlisting}[language=Python]
# Calculate linear regression on original data.
  original_auto = formula.ols("mpg \sim horsepower", data=auto_data).fit() 
# Create the bootstrap data set.
  bootstrap_auto = pd.DataFrame(boot_lm(auto_data, "mpg \sim horsepower", 1000)) 
\end{lstlisting}
We then run the code and print out the mean and error for the intercept and horsepower.
\begin{lstlisting}[language=Python]
original params
  Intercept:  mean:  39.9358610212
  Intercept:  error: 0.717498655555
  horsepower: mean:  -0.157844733354
  horsepower: error: 0.00644550051769
Bootstrapped params
  Intercept:  mean:  39.9805602832
  Intercept:  error: 0.0288069750935
  horsepower: mean:  -0.158352159577
  horsepower: error: 0.000244810973842
\end{lstlisting}

For the linear regression we get a very similar means for both the Intercept and horsepower, while the RSE has dropped sharply for both the intercept and horsepower. This shows that the original estimates of the parameters were quite good as most of the error in the estimate can be explained by noise in the data and not $\epsilon_i$ as the regression formula assumes.

For the end of the exercise we will run the same code, but using a quadratic term. This time the fit to the data is very good and we see similar results as the error drops by a factor of $\sim30$.

\begin{lstlisting}[language=Python]
# Calculate quadratic regression on the original data
  quad_auto = formula.ols("mpg ~ horsepower + np.power(horsepower, 2)", data=auto_data).fit()
# Create the bootstrap data set.
  bootstrap_quad_auto = pd.DataFrame(boot_lm(auto_data, "mpg ~ horsepower + np.power(horsepower, 2)", 1000))
\end{lstlisting}
\begin{lstlisting}[language=Python]
original params
 Intercept:  mean:               56.9000997021
 Intercept:  error:              1.80042680631
 horsepower: mean:               -0.466189629947
 horsepower: error:              0.0311246171196
 np.power(horsepower, 2): mean:  0.00123053610077
 np.power(horsepower, 2): error: 0.00012207586276
Bootstrapped params
 Intercept:  mean:               57.072651342
 Intercept:  error:              0.066630006277
 horsepower: mean:               -0.469465867184
 horsepower: error:              0.00106828698911
 np.power(horsepower, 2): mean:  0.00124365698568
 np.power(horsepower, 2): error: 3.87603834867e-06
\end{lstlisting}



